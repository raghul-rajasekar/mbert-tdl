{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import copy \n",
    "import csv \n",
    "import os \n",
    "import modeling \n",
    "import optimization \n",
    "import tokenization \n",
    "import tensorflow as tf\n",
    "import six\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_checkpoint = \"/home/raghul/Documents/CS7016/mbert-tdl/multi_cased_L-12_H-768_A-12/bert_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bert/embeddings/LayerNorm/beta', [768]),\n",
       " ('bert/embeddings/LayerNorm/gamma', [768]),\n",
       " ('bert/embeddings/position_embeddings', [512, 768]),\n",
       " ('bert/embeddings/token_type_embeddings', [2, 768]),\n",
       " ('bert/embeddings/word_embeddings', [119547, 768]),\n",
       " ('bert/encoder/layer_0/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_0/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_0/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_0/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_0/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_0/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_0/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_0/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_0/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_0/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_0/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_0/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_0/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_0/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_0/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_0/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_1/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_1/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_1/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_1/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_1/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_1/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_1/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_1/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_1/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_1/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_1/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_1/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_1/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_1/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_1/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_1/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_10/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_10/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_10/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_10/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_10/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_10/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_10/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_10/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_10/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_10/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_10/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_10/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_10/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_10/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_10/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_10/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_11/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_11/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_11/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_11/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_11/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_11/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_11/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_11/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_11/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_11/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_11/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_11/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_11/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_11/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_11/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_11/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_2/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_2/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_2/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_2/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_2/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_2/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_2/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_2/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_2/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_2/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_2/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_2/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_2/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_2/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_2/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_2/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_3/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_3/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_3/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_3/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_3/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_3/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_3/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_3/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_3/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_3/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_3/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_3/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_3/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_3/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_3/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_3/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_4/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_4/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_4/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_4/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_4/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_4/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_4/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_4/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_4/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_4/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_4/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_4/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_4/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_4/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_4/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_4/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_5/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_5/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_5/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_5/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_5/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_5/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_5/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_5/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_5/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_5/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_5/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_5/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_5/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_5/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_5/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_5/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_6/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_6/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_6/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_6/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_6/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_6/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_6/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_6/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_6/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_6/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_6/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_6/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_6/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_6/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_6/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_6/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_7/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_7/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_7/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_7/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_7/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_7/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_7/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_7/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_7/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_7/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_7/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_7/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_7/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_7/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_7/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_7/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_8/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_8/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_8/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_8/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_8/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_8/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_8/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_8/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_8/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_8/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_8/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_8/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_8/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_8/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_8/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_8/output/dense/kernel', [3072, 768]),\n",
       " ('bert/encoder/layer_9/attention/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_9/attention/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_9/attention/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_9/attention/output/dense/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_9/attention/self/key/bias', [768]),\n",
       " ('bert/encoder/layer_9/attention/self/key/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_9/attention/self/query/bias', [768]),\n",
       " ('bert/encoder/layer_9/attention/self/query/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_9/attention/self/value/bias', [768]),\n",
       " ('bert/encoder/layer_9/attention/self/value/kernel', [768, 768]),\n",
       " ('bert/encoder/layer_9/intermediate/dense/bias', [3072]),\n",
       " ('bert/encoder/layer_9/intermediate/dense/kernel', [768, 3072]),\n",
       " ('bert/encoder/layer_9/output/LayerNorm/beta', [768]),\n",
       " ('bert/encoder/layer_9/output/LayerNorm/gamma', [768]),\n",
       " ('bert/encoder/layer_9/output/dense/bias', [768]),\n",
       " ('bert/encoder/layer_9/output/dense/kernel', [3072, 768]),\n",
       " ('bert/pooler/dense/bias', [768]),\n",
       " ('bert/pooler/dense/kernel', [768, 768]),\n",
       " ('cls/predictions/output_bias', [119547]),\n",
       " ('cls/predictions/transform/LayerNorm/beta', [768]),\n",
       " ('cls/predictions/transform/LayerNorm/gamma', [768]),\n",
       " ('cls/predictions/transform/dense/bias', [768]),\n",
       " ('cls/predictions/transform/dense/kernel', [768, 768]),\n",
       " ('cls/seq_relationship/output_bias', [2]),\n",
       " ('cls/seq_relationship/output_weights', [2, 768])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars = tf.train.list_variables(init_checkpoint)\n",
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "    \"\"\"Constructs a InputExample.\n",
    "\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.guid = guid\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    self.label = label\n",
    "\n",
    "\n",
    "class PaddingInputExample(object):\n",
    "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               is_real_example=True):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    self.is_real_example = is_real_example\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @classmethod\n",
    "  def _read_tsv(cls, input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "      lines = []\n",
    "      for line in reader:\n",
    "        lines.append(line)\n",
    "      return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MrpcProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      if i == 0:\n",
    "        continue\n",
    "      guid = \"%s-%s\" % (set_type, i)\n",
    "      text_a = tokenization.convert_to_unicode(line[3])\n",
    "      text_b = tokenization.convert_to_unicode(line[4])\n",
    "      if set_type == \"test\":\n",
    "        label = \"0\"\n",
    "      else:\n",
    "        label = tokenization.convert_to_unicode(line[0])\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "  # This is a simple heuristic which will always truncate the longer sequence\n",
    "  # one token at a time. This makes more sense than truncating an equal percent\n",
    "  # of tokens from each, since if one sequence is very short then each token\n",
    "  # that's truncated likely contains more information than a longer sequence.\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_length:\n",
    "      break\n",
    "    if len(tokens_a) > len(tokens_b):\n",
    "      tokens_a.pop()\n",
    "    else:\n",
    "      tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "                           tokenizer):\n",
    "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "  if isinstance(example, PaddingInputExample):\n",
    "    return InputFeatures(\n",
    "        input_ids=[0] * max_seq_length,\n",
    "        input_mask=[0] * max_seq_length,\n",
    "        segment_ids=[0] * max_seq_length,\n",
    "        label_id=0,\n",
    "        is_real_example=False)\n",
    "\n",
    "  label_map = {}\n",
    "  for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "\n",
    "  tokens_a = tokenizer.tokenize(example.text_a)\n",
    "  tokens_b = None\n",
    "  if example.text_b:\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "  if tokens_b:\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "  else:\n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "  # The convention in BERT is:\n",
    "  # (a) For sequence pairs:\n",
    "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "  # (b) For single sequences:\n",
    "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "  #  type_ids: 0     0   0   0  0     0 0\n",
    "  #\n",
    "  # Where \"type_ids\" are used to indicate whether this is the first\n",
    "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "  # embedding vector (and position vector). This is not *strictly* necessary\n",
    "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "  # it easier for the model to learn the concept of sequences.\n",
    "  #\n",
    "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "  # used as the \"sentence vector\". Note that this only makes sense because\n",
    "  # the entire model is fine-tuned.\n",
    "  tokens = []\n",
    "  segment_ids = []\n",
    "  tokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "  tokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "\n",
    "  if tokens_b:\n",
    "    for token in tokens_b:\n",
    "      tokens.append(token)\n",
    "      segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "  # tokens are attended to.\n",
    "  input_mask = [1] * len(input_ids)\n",
    "\n",
    "  # Zero-pad up to the sequence length.\n",
    "  while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "\n",
    "  label_id = label_map[example.label]\n",
    "  if ex_index < 5:\n",
    "    tf.logging.info(\"*** Example ***\")\n",
    "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "        [tokenization.printable_text(x) for x in tokens]))\n",
    "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "  feature = InputFeatures(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids,\n",
    "      label_id=label_id,\n",
    "      is_real_example=True)\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer):\n",
    "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "  features = []\n",
    "  for (ex_index, example) in enumerate(examples):\n",
    "    if ex_index % 10000 == 0:\n",
    "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "    feature = convert_single_example(ex_index, example, label_list,\n",
    "                                     max_seq_length, tokenizer)\n",
    "\n",
    "    features.append(feature)\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MrpcProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base_dir = \"/home/raghul/Documents/CS7016/mbert-tdl/multi_cased_L-12_H-768_A-12\"\n",
    "glue_dir = \"/home/raghul/Documents/CS7016/mbert-tdl/glue_data\"\n",
    "\n",
    "bert_config_file = bert_base_dir + \"/bert_config.json\"\n",
    "vocab_file = bert_base_dir + \"/vocab.txt\"\n",
    "data_dir = glue_dir + \"/MRPC\"\n",
    "init_checkpoint = bert_base_dir + \"/bert_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = processor.get_train_examples(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 2\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-13\n",
      "INFO:tensorflow:tokens: [CLS] But he added group performance would improve in the second half of the year and beyond . [SEP] De Sole said in the results statement that group performance would improve in the second half of the year and beyond . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 16976 10261 16288 11795 14432 10894 33992 10106 10105 11132 13877 10108 10105 10924 10111 28569 119 102 10190 67478 12415 10106 10105 17466 33311 10189 11795 14432 10894 33992 10106 10105 11132 13877 10108 10105 10924 10111 28569 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-14\n",
      "INFO:tensorflow:tokens: [CLS] He told The Sun newspaper that Mr . Hussein ' s daughters had British schools and hospitals in mind when they decided to ask for as ##ylum . [SEP] \" Saddam ' s daughters had British schools and hospitals in mind when they decided to ask for as ##ylum - - especially the schools , \" he told The Sun . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 10357 21937 10117 13967 22047 10189 12916 119 49053 112 187 43392 10374 11160 16009 10111 70105 10106 21133 10841 10689 17270 10114 63001 10142 10146 54716 119 102 107 85431 112 187 43392 10374 11160 16009 10111 70105 10106 21133 10841 10689 17270 10114 63001 10142 10146 54716 118 118 17491 10105 16009 117 107 10261 21937 10117 13967 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.InputFeatures at 0x7fdaff82c550>,\n",
       " <__main__.InputFeatures at 0x7fdb0413fc18>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(train_examples[12:14], ['0', '1'], 128, tokenizer)\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "  model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids,\n",
    "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.convert_to_tensor([feature.input_ids for feature in train_features])\n",
    "input_mask = tf.convert_to_tensor([feature.input_mask for feature in train_features])\n",
    "segment_ids = tf.convert_to_tensor([feature.segment_ids for feature in train_features])\n",
    "label_ids = tf.convert_to_tensor([feature.label_id for feature in train_features])\n",
    "\n",
    "model = create_model(\n",
    "        bert_config, False, input_ids, input_mask, segment_ids, label_ids,\n",
    "        2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
    "                           seq_length, width):\n",
    "    output_tensor = tf.reshape(\n",
    "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
    "\n",
    "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(layer, model, config, batch_size, seq_length, token_type_ids, size_per_head, attention_mask):\n",
    "    with tf.variable_scope(\"layer_%d\" % layer, reuse = True):\n",
    "        with tf.variable_scope(\"attention\", reuse = True):\n",
    "            with tf.variable_scope(\"self\", reuse = True):\n",
    "                query_layer = tf.layers.dense(\n",
    "                  model.embedding_output,\n",
    "                  config.hidden_size,\n",
    "                  activation=None,\n",
    "                  name=\"query\")\n",
    "\n",
    "                # `key_layer` = [B*T, N*H]\n",
    "                key_layer = tf.layers.dense(\n",
    "                  model.embedding_output,\n",
    "                  config.hidden_size,\n",
    "                  activation=None,\n",
    "                  name=\"key\")\n",
    "\n",
    "                # `value_layer` = [B*T, N*H]\n",
    "                value_layer = tf.layers.dense(\n",
    "                  model.embedding_output,\n",
    "                  config.hidden_size,\n",
    "                  activation=None,\n",
    "                  name=\"value\")\n",
    "\n",
    "                # `query_layer` = [B, N, F, H]\n",
    "                query_layer = transpose_for_scores(query_layer, batch_size,\n",
    "                             config.num_attention_heads, seq_length,\n",
    "                             size_per_head)\n",
    "\n",
    "                # `key_layer` = [B, N, T, H]\n",
    "                key_layer = transpose_for_scores(key_layer, batch_size, config.num_attention_heads,\n",
    "                           seq_length, size_per_head)\n",
    "\n",
    "                # Take the dot product between \"query\" and \"key\" to get the raw\n",
    "                # attention scores.\n",
    "                # `attention_scores` = [B, N, F, T]\n",
    "                attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "                attention_scores = tf.multiply(attention_scores,\n",
    "                                               1.0 / math.sqrt(float(size_per_head)))\n",
    "                if attention_mask is not None:\n",
    "                  # `attention_mask` = [B, 1, F, T]\n",
    "                  attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
    "\n",
    "                  # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "                  # masked positions, this operation will create a tensor which is 0.0 for\n",
    "                  # positions we want to attend and -10000.0 for masked positions.\n",
    "                  adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
    "\n",
    "                  # Since we are adding it to the raw scores before the softmax, this is\n",
    "                  # effectively the same as removing these entirely.\n",
    "                  attention_scores += adder\n",
    "                # Normalize the attention scores to probabilities.\n",
    "                # `attention_probs` = [B, N, F, T]\n",
    "                attention_probs = tf.nn.softmax(attention_scores)\n",
    "                return attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = copy.deepcopy(bert_config)\n",
    "config.hidden_dropout_prob = 0.0\n",
    "config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "input_shape = modeling.get_shape_list(input_ids, expected_rank=2)\n",
    "batch_size = input_shape[0]\n",
    "seq_length = input_shape[1]\n",
    "\n",
    "token_type_ids = segment_ids\n",
    "\n",
    "size_per_head = int(config.hidden_size/config.num_attention_heads)\n",
    "\n",
    "attention_probs_all_layers = []\n",
    "\n",
    "with tf.variable_scope(\"bert\", reuse=True):\n",
    "  with tf.variable_scope(\"encoder\", reuse=True):\n",
    "        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n",
    "        # mask of shape [batch_size, seq_length, seq_length] which is used\n",
    "        # for the attention scores.\n",
    "        attention_mask = modeling.create_attention_mask_from_input_mask(\n",
    "            input_ids, input_mask)\n",
    "        for i in range(12):\n",
    "            attention_probs = get_attention_weights(i,\n",
    "                                                    model,\n",
    "                                                    config,\n",
    "                                                    batch_size,\n",
    "                                                    seq_length,\n",
    "                                                    token_type_ids,\n",
    "                                                    size_per_head,\n",
    "                                                    attention_mask)\n",
    "            attention_probs_all_layers.append(attention_probs)\n",
    "            \n",
    "attention_probs_all_layers = tf.stack(attention_probs_all_layers, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'bert/embeddings/word_embeddings:0' shape=(119547, 768) dtype=float32_ref>, <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref>, <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref>, <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref>, <tf.Variable 'bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32_ref>, <tf.Variable 'bert/pooler/dense/bias:0' shape=(768,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "print(tvars)\n",
    "\n",
    "(assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "tf.train.init_from_checkpoint(init_checkpoint, assignment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    probs = sess.run(attention_probs_all_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 12, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01225172, 0.01315385, 0.00439894, 0.00382379, 0.21517873,\n",
       "       0.00495163, 0.00187129, 0.01400201, 0.00470753, 0.01748784,\n",
       "       0.00187349, 0.00386996, 0.00365059, 0.00429015, 0.00672771,\n",
       "       0.0029812 , 0.00319304, 0.00128783, 0.00909213, 0.00652297,\n",
       "       0.00213521, 0.00420338, 0.00137108, 0.00107538, 0.00158061,\n",
       "       0.00447794, 0.00576828, 0.02096051, 0.00406371, 0.0077876 ,\n",
       "       0.00584981, 0.00653663, 0.0018854 , 0.00498847, 0.00528718,\n",
       "       0.00815405, 0.01259863, 0.00416539, 0.00449426, 0.00151985,\n",
       "       0.00984372, 0.00861141, 0.00418927, 0.00967541, 0.00376012,\n",
       "       0.00288444, 0.00270182, 0.00803264, 0.00995588, 0.02952315,\n",
       "       0.01972046, 0.022369  , 0.00357656, 0.00240939, 0.00479007,\n",
       "       0.00813237, 0.0056687 , 0.00800696, 0.00426274, 0.0043012 ,\n",
       "       0.36638713, 0.00581693, 0.01116088, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example number, layer, head, token number\n",
    "probs[1, 11, 11, 4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
